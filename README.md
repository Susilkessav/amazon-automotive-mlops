# End-to-End RAG-Based Q&A System for Amazon Automotive Reviews

An MLOps pipeline that ingests raw Amazon automotive review data, preprocesses & embeds it, serves a Retrieval-Augmented Generation (RAG) API + dashboard, and provides hooks for automated evaluation, CI/CD deployment, and monitoring.

---

## Architecture Overview

The system is organized into **five** logical layersâ€”each mapped to a folder or file in this repo:

| Layer                      | Components in This Repo                                      |
|----------------------------|---------------------------------------------------------------|
| **1. Data Ingestion**      | `dags/` (Airflow), `ingest_metadata.py`, `ingest_reviews.py`,<br>`dataset_processor.py` / `process_dataset_cli.py` |
| **2. Model Building**      | `app.py` (Flask API), `dashboard.py` (Flask UI),<br>`static/` + `templates/` |
| **3. Model Evaluation**    | (Future) Log-extraction & RAG-as-a-judge scripts & alerts*    |
| **4. CI/CD & Deployment**  | `Dockerfile.chatbot`, `docker-compose.yml`                   |
| **5. Monitoring & Logging**| Langfuse integration in `webserver_config.py`,<br>GCP Monitoring hooks |

> *_Note: The evaluation layer is scaffolded via Langfuse logs; you can plug in RAG-as-a-judge jobs in `scripts/` as needed._

---

## 1. Data Ingestion

Orchestrated by **Airflow DAGs** in `dags/` (requires `airflow.cfg`, `airflow.db`).

1. **Download / Unzip**  
   - Ingest raw CSV / JSON from S3 or local  
   - See `ingest_metadata.py`, `ingest_reviews.py`  

2. **Schema Validation & Cleaning**  
   - `dataset_processor.validate_and_clean()`  

3. **Data Transformation**  
   - Normalize fields, merge reviews + metadata  

4. **Vector Embedding Generation**  
   - MPNet via HuggingFace (`all-MiniLM-L6-v2`) in `dataset_processor.py`  
   - Outputs embeddings to local Chroma store (or GCS when `CHROMA_DB_DIR` points to a GCS bucket)  

5. **(Optional) Data Bias Report**  
   - Hook in your own report generator in the DAG  

---

## 2. Model Building

Exposed as a **Flask** microservice + dashboard:

- **`app.py`**  
  - `/chat` endpoint for RAG Q&A  
  - **Query Flow**:  
    1. UI (`dashboard.py`) â†’ User query  
    2. **Moderation** via OpenAI â†’  
    3. **RAG Retriever** (Chroma) â†’  
    4. **LLM** (OpenAI) â†’  
    5. **Langfuse** logs the request/response â†’  
    6. JSON response â†’ UI  

- **`dashboard.py`**  
  - Interactive UI for questions & visualizing results  
  - Templates in `templates/`, static assets in `static/`

- **Configuration**  
  - All API keys, GCS paths, Langfuse DSN â†’ `webserver_config.py` / `.env`

---

## 3. Model Evaluation

While real-time evaluation is plumbed through **Langfuse** (logs every RAG call), you can:

1. **Extract logs** (Langfuse SDK in `scripts/`)  
2. **Run RAG-as-a-judge** (LLM-based metrics)  
3. **Compare** against thresholds  
4. **Trigger** email alerts when performance dips  

> Starter scripts: `scripts/extract_logs.py`, `scripts/evaluate_rag_as_judge.py`

---

## 4. CI/CD & Deployment

- **Local / Dev**  
  ```bash
  docker-compose up --build -d
  ```
  - **Airflow** + **Flask Chatbot** spin up via `docker-compose.yml`  
  - Chatbot image built by `Dockerfile.chatbot`

- **Production Blueprint**  
  1. **GitHub â†’** push to `main`  
  2. **Build & Push** container to Artifact Registry  
  3. **Deploy** on GKE / Compute Engine  
  4. **On PR**: spin up ephemeral data-pipeline node, auto-destroy on close  

> Future GitHub Actions workï¬‚ows can live in `.github/workflows/`.

---

## 5. Monitoring & Logging

- **Langfuse**  
  - Tracks every API request/response  
  - View error rates, latencies, custom tags  

- **GCP Cloud Monitoring**  
  - Resource utilization dashboards (CPU, Memory)  
  - Alerting rules for above-threshold metrics â†’ Email notifications  

---

## Project Structure

```
.
â”œâ”€â”€ dags/                       # Airflow DAG definitions
â”œâ”€â”€ scripts/                    # Utilities: log extraction, evaluation, helpers
â”œâ”€â”€ static/ & templates/        # Flask dashboard UI
â”œâ”€â”€ Dockerfile.chatbot          # Builds RAG chatbot image
â”œâ”€â”€ docker-compose.yml          # Orchestrates Airflow + Chatbot
â”œâ”€â”€ app.py                      # Flask RAG API & Langfuse integration
â”œâ”€â”€ dashboard.py                # Flask UI server
â”œâ”€â”€ dataset_processor.py        # Validation, cleaning, merging & embedding logic
â”œâ”€â”€ ingest_metadata.py          # Metadata ingestion
â”œâ”€â”€ ingest_reviews.py           # Review ingestion
â”œâ”€â”€ process_dataset_cli.py      # CLI wrapper around dataset_processor
â”œâ”€â”€ webserver_config.py         # Config (.env loader, OpenAI, Langfuse, GCS)
â”œâ”€â”€ requirements.txt            # Python deps
â”œâ”€â”€ airflow.cfg & airflow.db    # Airflow config & metadata DB
â””â”€â”€ .env.example                # Copy to `.env` and fill in secrets
```

---

## Getting Started

1. **Clone & configure**  
   ```bash
   git clone <repo-url>
   cd <repo>
   cp .env.example .env
   # Fill in OPENAI_API_KEY, LANGFUSE_DSN, CHROMA_DB_DIR, etc.
   ```

2. **Install / Dockerize**  
   - **Local Python**  
     ```bash
     pip install -r requirements.txt
     airflow db init
     ```
   - **Docker Compose**  
     ```bash
     docker-compose up --build -d
     ```

3. **Airflow UI** â†’ `http://localhost:8080`  
   - Trigger DAGs: metadata â†’ reviews â†’ processing â†’ embeddings  

4. **Dashboard & API** â†’  
   - API: `http://localhost:5000`  
   - UI:  `http://localhost:5001`

5. **Monitor & Evaluate**  
   - View Langfuse dashboard  
   - Run evaluation scripts in `scripts/`

---

## ğŸ¤ Contributing

1. Fork & branch (`feat/...`, `fix/...`)  
2. Commit with descriptive messages  
3. Open a PR â†’ Review â†’ Merge  
